{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Tasks\n",
    "\n",
    "En el problemas de aprendizaje, a diferencia de en programación dinámica, no conocemos los detalles del problema. Por lo tanto vamos a modelar un problema de aprendizaje con dos \"clases\". \n",
    "\n",
    "1. La primera clase va a definir la tarea. Vamos a nombrar a esta tarea RLEnvironment\n",
    "2. La segunda clase define al agente que aprende a resolver la tarea maximizando su ganancia.\n",
    "\n",
    "Vamos a ver como construir cada una de estas clases de manera que tengamos un cascarón para distintas tareas de aprendizaje.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Una tarea de ejemplo: el problema el viajero\n",
    "\n",
    "Para poder entender como modelar un problema con estas clases. Vamos a programar un agente que aprenda a encontrar un camino optimo para visitar todas las ciudades de Europa empezando desde cualquier ciudad inicial.\n",
    "\n",
    "La informacion de distancia entre ciudades esta dada en la siguiente tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25×25 Array{Any,2}:\n",
       " \"Origin\"                \"Barcelona\"  …      \"Vienna\"      \"Warsaw\"\n",
       " \"Barcelona\"            0                1347.43       1862.33     \n",
       " \"Belgrade\"          1528.13              489.28        826.66     \n",
       " \"Berlin\"            1497.61              523.61        516.06     \n",
       " \"Brussels\"          1062.89              914.81       1159.85     \n",
       " \"Bucharest\"         1968.42          …   855.32        946.12     \n",
       " \"Budapest\"          1498.79              216.98        545.29     \n",
       " \"Copenhagen\"        1757.54              868.87        667.8      \n",
       " \"Dublin\"            1469.29             1680          1823.72     \n",
       " \"Hamburg\"           1471.78              742.79        750.49     \n",
       " \"Istanbul\"          2230.42          …  1273.88       1386.08     \n",
       " \"Kiev\"              2391.06             1052.76        690.12     \n",
       " \"London\"            1137.67             1233.48       1445.85     \n",
       " \"Madrid\"             504.64             1807.09       2288.42     \n",
       " \"Milan\"              725.12              623.36       1143.01     \n",
       " \"Moscow\"            3006.93          …  1669.22       1149.41     \n",
       " \"Munich\"            1054.55              354.42        809.02     \n",
       " \"Paris\"              831.59             1033.73       1365.91     \n",
       " \"Prague\"            1353.9               250.71        514.69     \n",
       " \"Rome\"               856.69              763.26       1316.24     \n",
       " \"Saint Petersburg\"  2813.02          …  1577.56       1023.41     \n",
       " \"Sofia\"             1745.55              817.45       1076.99     \n",
       " \"Stockholm\"         2276.51             1241.9         808.14     \n",
       " \"Vienna\"            1347.43                0           557.43     \n",
       " \"Warsaw\"            1862.33              557.43          0        "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_info = readcsv(\"data/DistMat.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las estrucutras basicas de Julia son los NamedArrays, que funcionan de manera similar a las matrices con nombres de dimensiones en R. Usaremos NamedArrays para guardar la informacion de distancias de manera que sea facil de accesar. Para poder usar NamedArrays necesitamos usar una paqueteria adicional a julia base con el comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pkg.add(\"NamedArrays\") # para instalar por primera vez\n",
    "using NamedArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuacion creamos un named array con la informacion de distancias y nombres dimenciones las ciudades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24×24 Named Array{Any,2}\n",
       "           A ╲ B │        Barcelona  …            Warsaw\n",
       "─────────────────┼──────────────────────────────────────\n",
       "Barcelona        │                0  …           1862.33\n",
       "Belgrade         │          1528.13               826.66\n",
       "Berlin           │          1497.61               516.06\n",
       "Brussels         │          1062.89              1159.85\n",
       "Bucharest        │          1968.42               946.12\n",
       "Budapest         │          1498.79               545.29\n",
       "Copenhagen       │          1757.54                667.8\n",
       "Dublin           │          1469.29              1823.72\n",
       "Hamburg          │          1471.78               750.49\n",
       "Istanbul         │          2230.42              1386.08\n",
       "Kiev             │          2391.06               690.12\n",
       "London           │          1137.67              1445.85\n",
       "Madrid           │           504.64              2288.42\n",
       "Milan            │           725.12              1143.01\n",
       "Moscow           │          3006.93              1149.41\n",
       "Munich           │          1054.55               809.02\n",
       "Paris            │           831.59              1365.91\n",
       "Prague           │           1353.9               514.69\n",
       "Rome             │           856.69              1316.24\n",
       "Saint Petersburg │          2813.02              1023.41\n",
       "Sofia            │          1745.55              1076.99\n",
       "Stockholm        │          2276.51               808.14\n",
       "Vienna           │          1347.43               557.43\n",
       "Warsaw           │          1862.33  …                 0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities = dist_info[2:end, 1] \n",
    "distances = dist_info[2:end, 2:end]\n",
    "distArray = NamedArray(distances, (cities, cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from Barcelona to Warsaw: 1862.33 km\n"
     ]
    }
   ],
   "source": [
    "d = distArray[\"Barcelona\", \"Warsaw\"]\n",
    "println(\"Distance from Barcelona to Warsaw: $d km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hubieramos podido implementar algo similar usando diccionarios, que son un tipo nativo de estructura de datos de julia y muchos otros lenguages. Pongo aqui la implementacion solo por curiosidad, aunque usaremos `distArray` para todos los calculos futuros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from Barcelona to Warsaw: 1862.33 km\n"
     ]
    }
   ],
   "source": [
    "distDict = Dict([([cities[i], cities[j]], distances[i, j]) for i in 1:24, j in 1:24])\n",
    "d = distDict[[\"Barcelona\", \"Warsaw\"]] # notemos los brackets adicionales\n",
    "println(\"Distance from Barcelona to Warsaw: $d km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Environments\n",
    "\n",
    "Los elementos que debe tener una tarea de aprendizaje son:\n",
    "\n",
    "1. Un espacio de estados\n",
    "2. Un espacio de acciones disponibles\n",
    "3. Un funcion de transicion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type RLEnv # reinforcement learning environment, should be immutable for efficiency\n",
    "    state_space::Array{Any, 1} # \n",
    "    action_space::Array{Any, 1} # \n",
    "    trans_fun::Function # (state, action) -> (new_state, reward)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, en nuestro problema el espacio de estados y de acciones es el mismo. Se puede estar en cualquier ciudad y una accion equivale a decidir a que ciudad irse. La funcion de transicion refleja el nuevo estado `s'` y el pago que `r` que un agente recibiria tras haber decidido una accion `a` estando en el estado `s`. Tanto `s'` como `r` pueden ser aleatorios. Veamos como seria una instanciacion para nuestro problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RLEnv(Any[\"Barcelona\",\"Belgrade\",\"Berlin\",\"Brussels\",\"Bucharest\",\"Budapest\",\"Copenhagen\",\"Dublin\",\"Hamburg\",\"Istanbul\"  …  \"Moscow\",\"Munich\",\"Paris\",\"Prague\",\"Rome\",\"Saint Petersburg\",\"Sofia\",\"Stockholm\",\"Vienna\",\"Warsaw\"],Any[\"Barcelona\",\"Belgrade\",\"Berlin\",\"Brussels\",\"Bucharest\",\"Budapest\",\"Copenhagen\",\"Dublin\",\"Hamburg\",\"Istanbul\"  …  \"Moscow\",\"Munich\",\"Paris\",\"Prague\",\"Rome\",\"Saint Petersburg\",\"Sofia\",\"Stockholm\",\"Vienna\",\"Warsaw\"],trans_fun)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_space = cities\n",
    "action_space = cities\n",
    "trans_fun(state, action) = action, - distArray[state, action] # la forma mas sencilla de definir funciones\n",
    "europe_tour = RLEnv(state_space, action_space, trans_fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, supongamos que actualmente estamos en Vienna y quiseramos ir a Warsaw. Entonces para europe tour la *accion* ir a Vienna dado que estamos en el *estado* Barcelona esta data por"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partiendo de Barcelona, se llego a la ciudad Paris obteniendo un pago de -831.59\n"
     ]
    }
   ],
   "source": [
    "new_state, reward = europe_tour.trans_fun(\"Barcelona\", \"Paris\")\n",
    "println(\"Partiendo de Barcelona, se llego a la ciudad $new_state obteniendo un pago de $reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentes que aprenden\n",
    "\n",
    "Para poder diseñar un agente necesitamos conocer el estado de un agente, la politica con la que toma decisiones y el pago total acumulado que ha recibido. Un agente solo esta definido dentro de una tarea de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type RLAgent\n",
    "    policy::Function # state -> action\n",
    "    state::Any # su estado actual\n",
    "    total_reward::Real # el reward que ha acumulado haste el momento\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que quede mas claro, vamos a crear un agente que en el problema del tour de europa se mueve al azar a cualquier otra ciudad partiendo de su estado actual. Vamos a suponer que el estado inicial es barcelona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition policy(Any) in module Main at In[10]:1 overwritten at In[12]:1.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RLAgent(policy,\"Barcelona\",0.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy(state) = rand(europe_tour.action_space) # el agente mas tonto escoge una ciudad al azar sin depender del estado actual\n",
    "state = \"Barcelona\"\n",
    "total_reward = 0.0 # empezamos con cero reward acumulado\n",
    "tour_agent = RLAgent(policy, state, total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La politica (aleatoria) que seguiria el agente tour_agent dado que actualmente se encuentra en\n",
      "Barcelona es ir a Belgrade\n"
     ]
    }
   ],
   "source": [
    "println(\"\"\"La politica (aleatoria) que seguiria el agente tour_agent dado que actualmente se encuentra en\n",
    "$(tour_agent.state) es ir a $(tour_agent.policy(tour_agent.state))\"\"\") # repetir este comando da distintos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diseñando la interaccion\n",
    "\n",
    "Para poder programar la interaccion entre un agente y su ambiente necesitamos programar *metodos*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando un agente interactua con el ambiente deben ocurrir las siguientes cosas:\n",
    "\n",
    "1. El agente escoge action data state segun su politica\n",
    "2. El ambiente transiciona aleatoriamente, generando un nuevo estado new_state al gente y dando un reward en el proceso\n",
    "\n",
    "Los metodos en julia son simplemente funciones cuyos argumentos son del tipo apropiado. A continuacion diseñamos un metodo de interaccion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "function interact!(agent::RLAgent, env::RLEnv)\n",
    "    # la convencion (opcional) de julia es incluir '!' al final de una funcion si modifica sus argumentos \n",
    "    new_state, reward = env.trans_fun(agent.state, agent.policy(agent.state))\n",
    "    agent.total_reward += reward\n",
    "    agent.state = new_state\n",
    "    return new_state, reward # a veces es conveniente guardar\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
