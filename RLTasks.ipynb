{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Tasks\n",
    "\n",
    "En el problemas de aprendizaje, a diferencia de en programación dinámica, no conocemos los detalles del problema. Por lo tanto vamos a modelar un problema de aprendizaje con dos \"clases\". \n",
    "\n",
    "1. La primera clase va a definir la tarea. Vamos a nombrar a esta tarea RLEnvironment\n",
    "2. La segunda clase define al agente que aprende a resolver la tarea maximizando su ganancia.\n",
    "\n",
    "Vamos a ver como construir cada una de estas clases de manera que tengamos un cascarón para distintas tareas de aprendizaje.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Una tarea de ejemplo: el problema el viajero\n",
    "\n",
    "Para poder entender como modelar un problema con estas clases. Vamos a programar un agente que aprenda a encontrar un camino optimo para visitar todas las ciudades de Europa empezando desde cualquier ciudad inicial.\n",
    "\n",
    "La informacion de distancia entre ciudades esta dada en la siguiente tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25×25 Array{Any,2}:\n",
       " \"Origin\"                \"Barcelona\"  …      \"Vienna\"      \"Warsaw\"\n",
       " \"Barcelona\"            0                1347.43       1862.33     \n",
       " \"Belgrade\"          1528.13              489.28        826.66     \n",
       " \"Berlin\"            1497.61              523.61        516.06     \n",
       " \"Brussels\"          1062.89              914.81       1159.85     \n",
       " \"Bucharest\"         1968.42          …   855.32        946.12     \n",
       " \"Budapest\"          1498.79              216.98        545.29     \n",
       " \"Copenhagen\"        1757.54              868.87        667.8      \n",
       " \"Dublin\"            1469.29             1680          1823.72     \n",
       " \"Hamburg\"           1471.78              742.79        750.49     \n",
       " \"Istanbul\"          2230.42          …  1273.88       1386.08     \n",
       " \"Kiev\"              2391.06             1052.76        690.12     \n",
       " \"London\"            1137.67             1233.48       1445.85     \n",
       " \"Madrid\"             504.64             1807.09       2288.42     \n",
       " \"Milan\"              725.12              623.36       1143.01     \n",
       " \"Moscow\"            3006.93          …  1669.22       1149.41     \n",
       " \"Munich\"            1054.55              354.42        809.02     \n",
       " \"Paris\"              831.59             1033.73       1365.91     \n",
       " \"Prague\"            1353.9               250.71        514.69     \n",
       " \"Rome\"               856.69              763.26       1316.24     \n",
       " \"Saint Petersburg\"  2813.02          …  1577.56       1023.41     \n",
       " \"Sofia\"             1745.55              817.45       1076.99     \n",
       " \"Stockholm\"         2276.51             1241.9         808.14     \n",
       " \"Vienna\"            1347.43                0           557.43     \n",
       " \"Warsaw\"            1862.33              557.43          0        "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_info = readcsv(\"data/DistMat.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las estrucutras basicas de Julia son los NamedArrays, que funcionan de manera similar a las matrices con nombres de dimensiones en R. Usaremos NamedArrays para guardar la informacion de distancias de manera que sea facil de accesar. Para poder usar NamedArrays necesitamos usar una paqueteria adicional a julia base con el comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pkg.add(\"NamedArrays\") # para instalar por primera vez\n",
    "using NamedArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuacion creamos un named array con la informacion de distancias y nombres dimenciones las ciudades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24×24 Named Array{Any,2}\n",
       "           A ╲ B │        Barcelona  …            Warsaw\n",
       "─────────────────┼──────────────────────────────────────\n",
       "Barcelona        │                0  …           1862.33\n",
       "Belgrade         │          1528.13               826.66\n",
       "Berlin           │          1497.61               516.06\n",
       "Brussels         │          1062.89              1159.85\n",
       "Bucharest        │          1968.42               946.12\n",
       "Budapest         │          1498.79               545.29\n",
       "Copenhagen       │          1757.54                667.8\n",
       "Dublin           │          1469.29              1823.72\n",
       "Hamburg          │          1471.78               750.49\n",
       "Istanbul         │          2230.42              1386.08\n",
       "Kiev             │          2391.06               690.12\n",
       "London           │          1137.67              1445.85\n",
       "Madrid           │           504.64              2288.42\n",
       "Milan            │           725.12              1143.01\n",
       "Moscow           │          3006.93              1149.41\n",
       "Munich           │          1054.55               809.02\n",
       "Paris            │           831.59              1365.91\n",
       "Prague           │           1353.9               514.69\n",
       "Rome             │           856.69              1316.24\n",
       "Saint Petersburg │          2813.02              1023.41\n",
       "Sofia            │          1745.55              1076.99\n",
       "Stockholm        │          2276.51               808.14\n",
       "Vienna           │          1347.43               557.43\n",
       "Warsaw           │          1862.33  …                 0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities = dist_info[2:end, 1] \n",
    "distances = dist_info[2:end, 2:end]\n",
    "distArray = NamedArray(distances, (cities, cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from Barcelona to Warsaw: 1862.33 km\n"
     ]
    }
   ],
   "source": [
    "d = distArray[\"Barcelona\", \"Warsaw\"]\n",
    "println(\"Distance from Barcelona to Warsaw: $d km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hubieramos podido implementar algo similar usando diccionarios, que son un tipo nativo de estructura de datos de julia y muchos otros lenguages. Pongo aqui la implementacion solo por curiosidad, aunque usaremos `distArray` para todos los calculos futuros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from Barcelona to Warsaw: 1862.33 km\n"
     ]
    }
   ],
   "source": [
    "distDict = Dict([([cities[i], cities[j]], distances[i, j]) for i in 1:24, j in 1:24])\n",
    "d = distDict[[\"Barcelona\", \"Warsaw\"]] # notemos los brackets adicionales\n",
    "println(\"Distance from Barcelona to Warsaw: $d km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Environments\n",
    "\n",
    "Los elementos que debe tener una tarea de aprendizaje son:\n",
    "\n",
    "1. Un espacio de estados\n",
    "2. Un espacio de acciones disponibles\n",
    "3. Un funcion de transicion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type RLEnv # reinforcement learning environment, should be immutable for efficiency\n",
    "    state_space::Array{Any, 1} # \n",
    "    trans_fun::Function # (state, action) -> (new_state, reward)\n",
    "    action_set::Function # (state) -> (array of available actions from state_space)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, en nuestro problema el espacio de estados y de acciones es el mismo. Se puede estar en cualquier ciudad y una accion equivale a decidir a que ciudad irse. La funcion de transicion refleja el nuevo estado `s'` y el pago que `r` que un agente recibiria tras haber decidido una accion `a` estando en el estado `s`. Tanto `s'` como `r` pueden ser aleatorios. Veamos como seria una instanciacion para nuestro problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RLEnv(Any[\"Barcelona\",\"Belgrade\",\"Berlin\",\"Brussels\",\"Bucharest\",\"Budapest\",\"Copenhagen\",\"Dublin\",\"Hamburg\",\"Istanbul\"  …  \"Moscow\",\"Munich\",\"Paris\",\"Prague\",\"Rome\",\"Saint Petersburg\",\"Sofia\",\"Stockholm\",\"Vienna\",\"Warsaw\"],trans_fun,action_set)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_space = cities\n",
    "trans_fun(state, action) = action, - distArray[state, action] # la forma mas sencilla de definir funciones\n",
    "action_set(state) = filter(x -> x ≠ state, cities)\n",
    "europe_tour = RLEnv(state_space, trans_fun, action_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, supongamos que actualmente estamos en Vienna y quiseramos ir a Warsaw. Entonces para europe tour la *accion* ir a Vienna dado que estamos en el *estado* Barcelona esta data por"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partiendo de Barcelona, se llego a la ciudad Paris obteniendo un pago de -831.59\n"
     ]
    }
   ],
   "source": [
    "new_state, reward = europe_tour.trans_fun(\"Barcelona\", \"Paris\")\n",
    "println(\"Partiendo de Barcelona, se llego a la ciudad $new_state obteniendo un pago de $reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentes que aprenden\n",
    "\n",
    "Para poder diseñar un agente necesitamos conocer el estado de un agente, la politica con la que toma decisiones y el pago total acumulado que ha recibido. Un agente solo esta definido dentro de una tarea de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type RLAgent\n",
    "    policy::Function # state -> action\n",
    "    state::Any # su estado actual\n",
    "    total_reward::Real # el reward que ha acumulado haste el momento\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que quede mas claro, vamos a crear un agente que en el problema del tour de europa se mueve al azar a cualquier otra ciudad partiendo de su estado actual. Vamos a suponer que el estado inicial es barcelona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RLAgent(policy,\"Barcelona\",0.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy(state) = rand(europe_tour.action_set(state)) # el agente mas tonto escoge una ciudad al azar sin depender del estado actual\n",
    "state = \"Barcelona\"\n",
    "total_reward = 0.0 # empezamos con cero reward acumulado\n",
    "tour_agent = RLAgent(policy, state, total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La politica (aleatoria) que seguiria el agente tour_agent dado que actualmente se encuentra en\n",
      "Barcelona es ir a Bucharest\n"
     ]
    }
   ],
   "source": [
    "println(\"\"\"La politica (aleatoria) que seguiria el agente tour_agent dado que actualmente se encuentra en\n",
    "$(tour_agent.state) es ir a $(tour_agent.policy(tour_agent.state))\"\"\") # repetir este comando da distintos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diseñando la interaccion\n",
    "\n",
    "Para poder programar la interaccion entre un agente y su ambiente necesitamos programar *metodos*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando un agente interactua con el ambiente deben ocurrir las siguientes cosas:\n",
    "\n",
    "1. El agente escoge action data state segun su politica\n",
    "2. El ambiente transiciona aleatoriamente, generando un nuevo estado new_state al gente y dando un reward en el proceso\n",
    "\n",
    "Los metodos en julia son simplemente funciones cuyos argumentos son del tipo apropiado. A continuacion diseñamos un metodo de interaccion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reset! (generic function with 2 methods)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function interact!(agent::RLAgent, env::RLEnv)\n",
    "    # la convencion (opcional) de julia es incluir '!' al final de una funcion si modifica sus argumentos \n",
    "    new_state, reward = env.trans_fun(agent.state, agent.policy(agent.state))\n",
    "    agent.total_reward += reward\n",
    "    agent.state = new_state\n",
    "    return new_state, reward # a veces es conveniente regresar los rewards de cada iteracion\n",
    "end\n",
    "function reset!(agent::RLAgent, state = agent.state) # el estado es opcional\n",
    "    agent.total_reward = 0\n",
    "    agent.state = state    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar simulemos 20 interacciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agente inicia en Barcelona con reward total 0\n",
      "Agente se mueve a Paris con reward total -831.59\n",
      "Agente se mueve a Hamburg con reward total -1576.22\n",
      "Agente se mueve a Dublin con reward total -2649.58\n",
      "Agente se mueve a Paris con reward total -3426.41\n",
      "Agente se mueve a Berlin con reward total -4303.37\n",
      "Agente se mueve a Milan con reward total -5144.09\n",
      "Agente se mueve a Belgrade con reward total -6029.41\n",
      "Agente se mueve a Berlin con reward total -7028.66\n",
      "Agente se mueve a Dublin con reward total -8343.82\n",
      "Agente se mueve a Belgrade con reward total -10489.21\n",
      "Agente se mueve a Dublin con reward total -12634.599999999999\n",
      "Agente se mueve a Moscow con reward total -15427.009999999998\n",
      "Agente se mueve a Madrid con reward total -18864.71\n",
      "Agente se mueve a London con reward total -20128.079999999998\n",
      "Agente se mueve a Barcelona con reward total -21265.75\n",
      "Agente se mueve a Rome con reward total -22122.44\n",
      "Agente se mueve a Kiev con reward total -23796.18\n",
      "Agente se mueve a Brussels con reward total -25632.38\n",
      "Agente se mueve a Belgrade con reward total -27004.97\n",
      "Agente se mueve a London con reward total -28693.940000000002\n"
     ]
    }
   ],
   "source": [
    "reset!(tour_agent, \"Barcelona\")\n",
    "println(\"Agente inicia en $(tour_agent.state) con reward total $(tour_agent.total_reward)\")\n",
    "for i in 1:20\n",
    "    interact!(tour_agent, europe_tour)\n",
    "    println(\"Agente se mueve a $(tour_agent.state) con reward total $(tour_agent.total_reward)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodios\n",
    "\n",
    "Un episodio termina cuando el agente visita todos los estados. Podemos programar una *Funcion* que simule un episodio usando nuestras funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "visit_all! (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function visit_all!(agent::RLAgent, env::RLEnv)\n",
    "    state_hist = [agent.state]\n",
    "    reward_hist = [0.0]\n",
    "    visited = Dict([(x, false) for x in env.state_space]) \n",
    "    while false in values(visited)\n",
    "        new_state, reward = interact!(agent, env)\n",
    "        push!(state_hist, new_state)\n",
    "        push!(reward_hist, reward) \n",
    "        visited[new_state] = true\n",
    "    end\n",
    "    state_hist, reward_hist\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 77 steps with total reward -93066.96999999999\n"
     ]
    }
   ],
   "source": [
    "reset!(tour_agent, \"Barcelona\")\n",
    "state_hist, reward_hist = visit_all!(tour_agent, europe_tour)\n",
    "println(\"Finished in $(size(state_hist, 1)) steps with total reward $(tour_agent.total_reward)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "policy_eval (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Policy evaluation\n",
    "function policy_eval(agent::RLAgent, env::RLEnv, sweeps)\n",
    "    state_space = env.state_space\n",
    "    value = Dict([(x, 0.0) for x in state_space])\n",
    "    for state in state_space, iter in 1:sweeps\n",
    "        reset!(agent, state)\n",
    "        visit_all!(agent, env)\n",
    "        value[state] += (agent.total_reward - value[state]) / iter\n",
    "    end\n",
    "    return value\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{SubString{String},Float64} with 24 entries:\n",
       "  \"Budapest\"         => -1.1891e5\n",
       "  \"Warsaw\"           => -1.11336e5\n",
       "  \"Rome\"             => -1.17975e5\n",
       "  \"Munich\"           => -1.18742e5\n",
       "  \"Kiev\"             => -1.0652e5\n",
       "  \"Prague\"           => -1.05644e5\n",
       "  \"Berlin\"           => -107950.0\n",
       "  \"Istanbul\"         => -124779.0\n",
       "  \"Madrid\"           => -1.06871e5\n",
       "  \"Barcelona\"        => -101311.0\n",
       "  \"Moscow\"           => -1.16186e5\n",
       "  \"Dublin\"           => -1.15444e5\n",
       "  \"Milan\"            => -113154.0\n",
       "  \"Sofia\"            => -1.09727e5\n",
       "  \"Vienna\"           => -1.17511e5\n",
       "  \"Brussels\"         => -1.09154e5\n",
       "  \"London\"           => -1.14859e5\n",
       "  \"Saint Petersburg\" => -1.11547e5\n",
       "  \"Copenhagen\"       => -1.10915e5\n",
       "  \"Paris\"            => -1.17799e5\n",
       "  \"Stockholm\"        => -1.20112e5\n",
       "  \"Belgrade\"         => -1.09022e5\n",
       "  \"Bucharest\"        => -1.02303e5\n",
       "  \"Hamburg\"          => -99511.5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_vals = policy_eval(tour_agent, europe_tour, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::#9) (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Policy improvement\n",
    "new_policy = function(state) \n",
    "    # En este ejemplo: q(state,a) = distance(state, a) + v(a) \n",
    "    # Esgemos la politica greedy pi*(state) = argmax_a q(state, a)\n",
    "    qx = [state_vals[state] - distArray[state, a] for a in cities if a ≠ state]\n",
    "    maxval, pos = findmax(qx)\n",
    "    return cities[pos]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Brussels\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_policy(\"Paris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.1",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
